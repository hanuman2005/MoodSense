{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5892ff1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Image Emotion Model Training\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook trains a CNN-based model for facial emotion recognition using MobileNetV2.\"\n",
    "   ]\\n  }\\n ],\\n \\\"metadata\\\": {\\n  \\\"kernelspec\\\": {\\n   \\\"display_name\\\": \\\"Python 3\\\",\\n   \\\"language\\\": \\\"python\\\",\\n   \\\"name\\\": \\\"python3\\\"\\n  },\\n  \\\"language_info\\\": {\\n   \\\"codemirror_mode\\\": {\\n    \\\"name\\\": \\\"ipython\\\",\\n    \\\"version\\\": 3\\n   },\\n   \\\"file_extension\\\": \\\".py\\\",\\n   \\\"mimetype\\\": \\\"text/x-python\\\",\\n   \\\"name\\\": \\\"python\\\",\\n   \\\"nbconvert_exporter\\\": \\\"python\\\",\\n   \\\"pygments_lexer\\\": \\\"ipython3\\\",\\n   \\\"version\\\": \\\"3.8.5\\\"\\n  }\\n },\\n \\\"nbformat\\\": 4,\\n \\\"nbformat_minor\\\": 4\\n}\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import sys\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"sys.path.append('../src')\\n\",\n",
    "    \"\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import torch.nn as nn\\n\",\n",
    "    \"import torchvision.transforms as transforms\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import cv2\\n\",\n",
    "    \"from PIL import Image\\n\",\n",
    "    \"from sklearn.metrics import classification_report, confusion_matrix\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"from data_loader import DataLoader\\n\",\n",
    "    \"from model_trainer import ModelTrainer, ImageEmotionModel\\n\",\n",
    "    \"from utils import EmotionUtils, ModelUtils, ImageUtils\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set random seeds for reproducibility\\n\",\n",
    "    \"torch.manual_seed(42)\\n\",\n",
    "    \"np.random.seed(42)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Check GPU availability\\n\",\n",
    "    \"device = ModelUtils.get_device()\\n\",\n",
    "    \"print(f\\\"Using device: {device}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Load and Prepare Image Data\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Initialize data loader\\n\",\n",
    "    \"data_loader = DataLoader(data_dir='../data/')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load processed image data\\n\",\n",
    "    \"processed_data_path = '../data/processed/image_emotions.csv'\\n\",\n",
    "    \"\\n\",\n",
    "    \"if os.path.exists(processed_data_path):\\n\",\n",
    "    \"    df = pd.read_csv(processed_data_path)\\n\",\n",
    "    \"    image_paths = df['image_path'].tolist()\\n\",\n",
    "    \"    labels = df['emotion'].tolist()\\n\",\n",
    "    \"    print(f\\\"Loaded {len(image_paths)} image samples from processed data\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"Processed data not found. Loading raw data...\\\")\\n\",\n",
    "    \"    # Try to load from FER-2013\\n\",\n",
    "    \"    fer_data_path = '../data/fer2013/'\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if os.path.exists(fer_data_path):\\n\",\n",
    "    \"        image_paths, labels = data_loader.load_fer2013_data(fer_data_path)\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(\\\"FER-2013 dataset not found. Creating synthetic data for demonstration...\\\")\\n\",\n",
    "    \"        # Create synthetic image data for demonstration\\n\",\n",
    "    \"        # In real scenario, you would have actual image files\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Create sample images directory\\n\",\n",
    "    \"        os.makedirs('../data/sample_images', exist_ok=True)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        image_paths = []\\n\",\n",
    "    \"        labels = []\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Generate sample grayscale images for each emotion\\n\",\n",
    "    \"        for emotion in EmotionUtils.EMOTION_LABELS:\\n\",\n",
    "    \"            for i in range(20):  # 20 samples per emotion\\n\",\n",
    "    \"                # Create a random 48x48 grayscale image\\n\",\n",
    "    \"                img = np.random.randint(0, 256, (48, 48), dtype=np.uint8)\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Add some pattern based on emotion (just for demo)\\n\",\n",
    "    \"                if emotion == 'happy':\\n\",\n",
    "    \"                    img = np.clip(img + 50, 0, 255)  # Brighter\\n\",\n",
    "    \"                elif emotion == 'sad':\\n\",\n",
    "    \"                    img = np.clip(img - 50, 0, 255)  # Darker\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                img_path = f'../data/sample_images/{emotion}_{i:03d}.png'\\n\",\n",
    "    \"                cv2.imwrite(img_path, img)\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                image_paths.append(img_path)\\n\",\n",
    "    \"                labels.append(emotion)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(f\\\"Created {len(image_paths)} synthetic image samples for training\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nUnique emotions: {set(labels)}\\\")\\n\",\n",
    "    \"print(f\\\"Label distribution:\\\")\\n\",\n",
    "    \"from collections import Counter\\n\",\n",
    "    \"label_counts = Counter(labels)\\n\",\n",
    "    \"for emotion, count in label_counts.items():\\n\",\n",
    "    \"    print(f\\\"  {emotion}: {count}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Filter out non-existent image paths\\n\",\n",
    "    \"valid_paths = []\\n\",\n",
    "    \"valid_labels = []\\n\",\n",
    "    \"for path, label in zip(image_paths, labels):\\n\",\n",
    "    \"    if os.path.exists(path):\\n\",\n",
    "    \"        valid_paths.append(path)\\n\",\n",
    "    \"        valid_labels.append(label)\\n\",\n",
    "    \"\\n\",\n",
    "    \"image_paths = valid_paths\\n\",\n",
    "    \"labels = valid_labels\\n\",\n",
    "    \"print(f\\\"\\\\nValid image files: {len(image_paths)}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Visualize sample images\\n\",\n",
    "    \"print(\\\"Sample Images:\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Select a few images from each emotion\\n\",\n",
    "    \"sample_images = {}\\n\",\n",
    "    \"for emotion in EmotionUtils.EMOTION_LABELS:\\n\",\n",
    "    \"    emotion_paths = [path for path, label in zip(image_paths, labels) if label == emotion]\\n\",\n",
    "    \"    if emotion_paths:\\n\",\n",
    "    \"        sample_images[emotion] = emotion_paths[0]  # Take first image of each emotion\\n\",\n",
    "    \"\\n\",\n",
    "    \"if sample_images:\\n\",\n",
    "    \"    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\\n\",\n",
    "    \"    axes = axes.flatten()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for i, (emotion, img_path) in enumerate(sample_images.items()):\\n\",\n",
    "    \"        if i < 8:  # Only show first 8\\n\",\n",
    "    \"            try:\\n\",\n",
    "    \"                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\\n\",\n",
    "    \"                if img is not None:\\n\",\n",
    "    \"                    axes[i].imshow(img, cmap='gray')\\n\",\n",
    "    \"                    axes[i].set_title(f'{emotion.capitalize()}')\\n\",\n",
    "    \"                    axes[i].axis('off')\\n\",\n",
    "    \"                else:\\n\",\n",
    "    \"                    axes[i].text(0.5, 0.5, f'{emotion}\\\\n(No image)', \\n\",\n",
    "    \"                               ha='center', va='center', transform=axes[i].transAxes)\\n\",\n",
    "    \"                    axes[i].axis('off')\\n\",\n",
    "    \"            except Exception as e:\\n\",\n",
    "    \"                axes[i].text(0.5, 0.5, f'{emotion}\\\\n(Error loading)', \\n\",\n",
    "    \"                           ha='center', va='center', transform=axes[i].transAxes)\\n\",\n",
    "    \"                axes[i].axis('off')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.suptitle('Sample Images by Emotion')\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\nelse:\\n\",\n",
    "    \"    print(\\\"No sample images available to display.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create data loaders\\n\",\n",
    "    \"print(\\\"Creating data loaders...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Configuration\\n\",\n",
    "    \"BATCH_SIZE = 32\\n\",\n",
    "    \"TEST_SIZE = 0.2\\n\",\n",
    "    \"IMAGE_SIZE = (48, 48)\\n\",\n",
    "    \"\\n\",\n",
    "    \"if len(image_paths) > 0:\\n\",\n",
    "    \"    train_loader, val_loader, label_encoder = data_loader.create_image_dataloaders(\\n\",\n",
    "    \"        image_paths=image_paths,\\n\",\n",
    "    \"        labels=labels,\\n\",\n",
    "    \"        batch_size=BATCH_SIZE,\\n\",\n",
    "    \"        test_size=TEST_SIZE\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"Training batches: {len(train_loader)}\\\")\\n\",\n",
    "    \"    print(f\\\"Validation batches: {len(val_loader)}\\\")\\n\",\n",
    "    \"    print(f\\\"Classes: {label_encoder.classes_}\\\")\\nelse:\\n\",\n",
    "    \"    print(\\\"‚ùå No valid image data available for training\\\")\\n\",\n",
    "    \"    train_loader = None\\n\",\n",
    "    \"    val_loader = None\\n\",\n",
    "    \"    label_encoder = None\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Initialize Model and Trainer\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Initialize model trainer\\n\",\n",
    "    \"model_trainer = ModelTrainer(device=device)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Training configuration\\n\",\n",
    "    \"training_config = {\\n\",\n",
    "    \"    \\\"model_architecture\\\": \\\"MobileNetV2\\\",\\n\",\n",
    "    \"    \\\"num_epochs\\\": 10,  # Reduced for demo\\n\",\n",
    "    \"    \\\"learning_rate\\\": 0.001,\\n\",\n",
    "    \"    \\\"batch_size\\\": BATCH_SIZE,\\n\",\n",
    "    \"    \\\"image_size\\\": IMAGE_SIZE,\\n\",\n",
    "    \"    \\\"num_emotions\\\": len(EmotionUtils.EMOTION_LABELS),\\n\",\n",
    "    \"    \\\"device\\\": str(device),\\n\",\n",
    "    \"    \\\"pretrained\\\": True\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Training Configuration:\\\")\\n\",\n",
    "    \"for key, value in training_config.items():\\n\",\n",
    "    \"    print(f\\\"  {key}: {value}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save training config\\n\",\n",
    "    \"os.makedirs('../models/image_emotion_model', exist_ok=True)\\n\",\n",
    "    \"model_trainer.save_training_config(training_config, '../models/image_emotion_model/config.json')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Train the Model\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Train the image emotion model\\n\",\n",
    "    \"if train_loader is not None and val_loader is not None:\\n\",\n",
    "    \"    print(\\\"Starting model training...\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    model_save_path = '../models/image_emotion_model/model.pth'\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    history = model_trainer.train_image_model(\\n\",\n",
    "    \"        train_loader=train_loader,\\n\",\n",
    "    \"        val_loader=val_loader,\\n\",\n",
    "    \"        model_save_path=model_save_path,\\n\",\n",
    "    \"        num_epochs=training_config[\\\"num_epochs\\\"],\\n\",\n",
    "    \"        learning_rate=training_config[\\\"learning_rate\\\"]\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(\\\"\\\\n‚úÖ Training completed!\\\")\\nelse:\\n\",\n",
    "    \"    print(\\\"‚ùå Cannot start training - no valid data available\\\")\\n\",\n",
    "    \"    # Create a dummy history for demonstration\\n\",\n",
    "    \"    history = {\\n\",\n",
    "    \"        'train_loss': [0.8, 0.6, 0.4, 0.3, 0.2],\\n\",\n",
    "    \"        'val_loss': [0.9, 0.7, 0.5, 0.4, 0.35],\\n\",\n",
    "    \"        'train_acc': [60, 70, 80, 85, 90],\\n\",\n",
    "    \"        'val_acc': [55, 65, 75, 80, 82]\\n\",\n",
    "    \"    }\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Visualize Training Results\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Plot training history\\n\",\n",
    "    \"if 'history' in locals():\\n\",\n",
    "    \"    model_trainer.plot_training_history(\\n\",\n",
    "    \"        history, \\n\",\n",
    "    \"        save_path='../models/image_emotion_model/training_history.png'\\n\",\n",
    "    \"    )\\nelse:\\n\",\n",
    "    \"    print(\\\"No training history available to plot\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Evaluate the Model\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load and evaluate the trained model\\n\",\n",
    "    \"model_save_path = '../models/image_emotion_model/model.pth'\\n\",\n",
    "    \"\\n\",\n",
    "    \"if os.path.exists(model_save_path) and val_loader is not None:\\n\",\n",
    "    \"    # Load the best trained model\\n\",\n",
    "    \"    best_model = model_trainer.load_trained_model(model_save_path, \\\"image\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if best_model is not None:\\n\",\n",
    "    \"        print(\\\"Model loaded successfully!\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Evaluate on validation set\\n\",\n",
    "    \"        print(\\\"\\\\nEvaluating model...\\\")\\n\",\n",
    "    \"        evaluation_results = model_trainer.evaluate_model(\\n\",\n",
    "    \"            model=best_model,\\n\",\n",
    "    \"            test_loader=val_loader,\\n\",\n",
    "    \"            model_type=\\\"image\\\"\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(f\\\"\\\\nValidation Accuracy: {evaluation_results['accuracy']:.4f}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Print classification report\\n\",\n",
    "    \"        print(\\\"\\\\nClassification Report:\\\")\\n\",\n",
    "    \"        report = evaluation_results['classification_report']\\n\",\n",
    "    \"        for emotion in EmotionUtils.EMOTION_LABELS:\\n\",\n",
    "    \"            if emotion in report:\\n\",\n",
    "    \"                metrics = report[emotion]\\n\",\n",
    "    \"                print(f\\\"  {emotion}:\\\")\\n\",\n",
    "    \"                print(f\\\"    Precision: {metrics['precision']:.3f}\\\")\\n\",\n",
    "    \"                print(f\\\"    Recall: {metrics['recall']:.3f}\\\")\\n\",\n",
    "    \"                print(f\\\"    F1-score: {metrics['f1-score']:.3f}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Plot confusion matrix\\n\",\n",
    "    \"        model_trainer.plot_confusion_matrix(\\n\",\n",
    "    \"            evaluation_results['confusion_matrix'],\\n\",\n",
    "    \"            save_path='../models/image_emotion_model/confusion_matrix.png'\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(\\\"‚ùå Failed to load trained model\\\")\\nelse:\\n\",\n",
    "    \"    print(\\\"‚ùå No trained model available or no validation data\\\")\\n\",\n",
    "    \"    best_model = None\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Test Model with Sample Images\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Test the model with sample images\\n\",\n",
    "    \"if best_model is not None and len(image_paths) > 0:\\n\",\n",
    "    \"    print(\\\"Testing model with sample images:\\\\n\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Select a few test images\\n\",\n",
    "    \"    test_indices = np.random.choice(len(image_paths), min(5, len(image_paths)), replace=False)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    fig, axes = plt.subplots(1, len(test_indices), figsize=(15, 3))\\n\",\n",
    "    \"    if len(test_indices) == 1:\\n\",\n",
    "    \"        axes = [axes]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for i, idx in enumerate(test_indices):\\n\",\n",
    "    \"        img_path = image_paths[idx]\\n\",\n",
    "    \"        true_label = labels[idx]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Load and preprocess image\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\\n\",\n",
    "    \"            if img is not None:\\n\",\n",
    "    \"                # Predict emotions\\n\",\n",
    "    \"                emotion_scores = model_trainer.predict_single_image(\\n\",\n",
    "    \"                    model=best_model,\\n\",\n",
    "    \"                    image=img\\n\",\n",
    "    \"                )\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Get predicted emotion\\n\",\n",
    "    \"                predicted_emotion = max(emotion_scores.items(), key=lambda x: x[1])[0]\\n\",\n",
    "    \"                confidence = emotion_scores[predicted_emotion]\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Display image\\n\",\n",
    "    \"                axes[i].imshow(img, cmap='gray')\\n\",\n",
    "    \"                axes[i].set_title(f'True: {true_label}\\\\nPred: {predicted_emotion}\\\\n({confidence:.2f})')\\n\",\n",
    "    \"                axes[i].axis('off')\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                print(f\\\"Image {i+1}: {img_path}\\\")\\n\",\n",
    "    \"                print(f\\\"  True emotion: {true_label}\\\")\\n\",\n",
    "    \"                print(f\\\"  Predicted emotion: {predicted_emotion} (confidence: {confidence:.3f})\\\")\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Show top 3 predictions\\n\",\n",
    "    \"                sorted_emotions = sorted(emotion_scores.items(), key=lambda x: x[1], reverse=True)[:3]\\n\",\n",
    "    \"                print(\\\"  Top predictions:\\\")\\n\",\n",
    "    \"                for emotion, score in sorted_emotions:\\n\",\n",
    "    \"                    emoji = EmotionUtils.get_emotion_emoji(emotion)\\n\",\n",
    "    \"                    print(f\\\"    {emoji} {emotion}: {score:.3f}\\\")\\n\",\n",
    "    \"                print()\\n\",\n",
    "    \"            else:\\n\",\n",
    "    \"                axes[i].text(0.5, 0.5, 'Failed to\\\\nload image', \\n\",\n",
    "    \"                           ha='center', va='center', transform=axes[i].transAxes)\\n\",\n",
    "    \"                axes[i].axis('off')\\n\",\n",
    "    \"        except Exception as e:\\n\",\n",
    "    \"            print(f\\\"Error processing image {img_path}: {e}\\\")\\n\",\n",
    "    \"            axes[i].text(0.5, 0.5, f'Error:\\\\n{str(e)[:20]}', \\n\",\n",
    "    \"                       ha='center', va='center', transform=axes[i].transAxes)\\n\",\n",
    "    \"            axes[i].axis('off')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\nelse:\\n\",\n",
    "    \"    print(\\\"‚ùå Model not available for testing or no images available\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. Model Performance Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze model performance by emotion\\n\",\n",
    "    \"if 'evaluation_results' in locals():\\n\",\n",
    "    \"    print(\\\"Model Performance Analysis:\\\\n\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Extract metrics for each emotion\\n\",\n",
    "    \"    emotions_data = []\\n\",\n",
    "    \"    report = evaluation_results['classification_report']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for emotion in EmotionUtils.EMOTION_LABELS:\\n\",\n",
    "    \"        if emotion in report:\\n\",\n",
    "    \"            emotions_data.append({\\n\",\n",
    "    \"                'emotion': emotion,\\n\",\n",
    "    \"                'precision': report[emotion]['precision'],\\n\",\n",
    "    \"                'recall': report[emotion]['recall'],\\n\",\n",
    "    \"                'f1_score': report[emotion]['f1-score'],\\n\",\n",
    "    \"                'support': report[emotion]['support']\\n\",\n",
    "    \"            })\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create DataFrame for easier analysis\\n\",\n",
    "    \"    if emotions_data:\\n\",\n",
    "    \"        performance_df = pd.DataFrame(emotions_data)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(\\\"Per-Emotion Performance:\\\")\\n\",\n",
    "    \"        print(performance_df.round(3))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Visualize performance metrics\\n\",\n",
    "    \"        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Precision\\n\",\n",
    "    \"        axes[0].bar(performance_df['emotion'], performance_df['precision'], \\n\",\n",
    "    \"                   color=[EmotionUtils.EMOTION_COLORS.get(e, '#888888') for e in performance_df['emotion']])\\n\",\n",
    "    \"        axes[0].set_title('Precision by Emotion')\\n\",\n",
    "    \"        axes[0].set_ylabel('Precision')\\n\",\n",
    "    \"        axes[0].tick_params(axis='x', rotation=45)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Recall\\n\",\n",
    "    \"        axes[1].bar(performance_df['emotion'], performance_df['recall'],\\n\",\n",
    "    \"                   color=[EmotionUtils.EMOTION_COLORS.get(e, '#888888') for e in performance_df['emotion']])\\n\",\n",
    "    \"        axes[1].set_title('Recall by Emotion')\\n\",\n",
    "    \"        axes[1].set_ylabel('Recall')\\n\",\n",
    "    \"        axes[1].tick_params(axis='x', rotation=45)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # F1-Score\\n\",\n",
    "    \"        axes[2].bar(performance_df['emotion'], performance_df['f1_score'],\\n\",\n",
    "    \"                   color=[EmotionUtils.EMOTION_COLORS.get(e, '#888888') for e in performance_df['emotion']])\\n\",\n",
    "    \"        axes[2].set_title('F1-Score by Emotion')\\n\",\n",
    "    \"        axes[2].set_ylabel('F1-Score')\\n\",\n",
    "    \"        axes[2].tick_params(axis='x', rotation=45)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.tight_layout()\\n\",\n",
    "    \"        plt.savefig('../models/image_emotion_model/performance_analysis.png')\\n\",\n",
    "    \"        plt.show()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Best and worst performing emotions\\n\",\n",
    "    \"        best_f1 = performance_df.loc[performance_df['f1_score'].idxmax()]\\n\",\n",
    "    \"        worst_f1 = performance_df.loc[performance_df['f1_score'].idxmin()]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(f\\\"\\\\nüèÜ Best performing emotion: {best_f1['emotion']} (F1: {best_f1['f1_score']:.3f})\\\")\\n\",\n",
    "    \"        print(f\\\"üî¥ Worst performing emotion: {worst_f1['emotion']} (F1: {worst_f1['f1_score']:.3f})\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Overall metrics\\n\",\n",
    "    \"        if 'macro avg' in report:\\n\",\n",
    "    \"            macro_avg = report['macro avg']\\n\",\n",
    "    \"            print(f\\\"\\\\nüìä Overall Performance:\\\")\\n\",\n",
    "    \"            print(f\\\"   Macro Precision: {macro_avg['precision']:.3f}\\\")\\n\",\n",
    "    \"            print(f\\\"   Macro Recall: {macro_avg['recall']:.3f}\\\")\\n\",\n",
    "    \"            print(f\\\"   Macro F1-Score: {macro_avg['f1-score']:.3f}\\\")\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(\\\"No emotion-specific performance data available\\\")\\nelse:\\n\",\n",
    "    \"    print(\\\"No evaluation results available for analysis\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 8. Save Model Summary\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create model summary\\n\",\n",
    "    \"model_summary = {\\n\",\n",
    "    \"    \\\"model_type\\\": \\\"Image Emotion Recognition\\\",\\n\",\n",
    "    \"    \\\"architecture\\\": \\\"MobileNetV2 + Classification Head\\\",\\n\",\n",
    "    \"    \\\"training_data\\\": {\\n\",\n",
    "    \"        \\\"total_samples\\\": len(image_paths) if 'image_paths' in locals() else 0,\\n\",\n",
    "    \"        \\\"training_samples\\\": len(train_loader.dataset) if train_loader else 0,\\n\",\n",
    "    \"        \\\"validation_samples\\\": len(val_loader.dataset) if val_loader else 0,\\n\",\n",
    "    \"        \\\"emotions\\\": list(EmotionUtils.EMOTION_LABELS),\\n\",\n",
    "    \"        \\\"image_size\\\": IMAGE_SIZE\\n\",\n",
    "    \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d97767b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from data_loader import DataLoader\n",
    "from model_trainer import ModelTrainer, ImageEmotionModel\n",
    "from utils import EmotionUtils, ModelUtils, ImageUtils\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = ModelUtils.get_device()\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb494740",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bb659d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "data_loader = DataLoader(data_dir='../data/')\n",
    "\n",
    "# Load processed image data\n",
    "processed_data_path = '../data/processed/image_emotions.csv'\n",
    "\n",
    "if os.path.exists(processed_data_path):\n",
    "    df = pd.read_csv(processed_data_path)\n",
    "    image_paths = df['image_path'].tolist()\n",
    "    labels = df['emotion'].tolist()\n",
    "    print(f\"Loaded {len(image_paths)} image samples from processed data\")\n",
    "else:\n",
    "    print(\"Processed data not found. Loading raw data...\")\n",
    "    # Try to load from FER-2013\n",
    "    fer_data_path = '../data/fer2013/'\n",
    "    \n",
    "    if os.path.exists(fer_data_path):\n",
    "        image_paths, labels = data_loader.load_fer2013_data(fer_data_path)\n",
    "    else:\n",
    "        print(\"FER-2013 dataset not found. Creating synthetic data for demonstration...\")\n",
    "        # Create synthetic image data for demonstration\n",
    "        # In real scenario, you would have actual image files\n",
    "        \n",
    "        # Create sample images directory\n",
    "        os.makedirs('../data/sample_images', exist_ok=True)\n",
    "        \n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        # Generate sample grayscale images for each emotion\n",
    "        for emotion in EmotionUtils.EMOTION_LABELS:\n",
    "            for i in range(20):  # 20 samples per emotion\n",
    "                # Create a random 48x48 grayscale image\n",
    "                img = np.random.randint(0, 256, (48, 48), dtype=np.uint8)\n",
    "                \n",
    "                # Add some pattern based on emotion (just for demo)\n",
    "                if emotion == 'happy':\n",
    "                    img = np.clip(img + 50, 0, 255)  # Brighter\n",
    "                elif emotion == 'sad':\n",
    "                    img = np.clip(img - 50, 0, 255)  # Darker\n",
    "                \n",
    "                img_path = f'../data/sample_images/{emotion}_{i:03d}.png'\n",
    "                cv2.imwrite(img_path, img)\n",
    "                \n",
    "                image_paths.append(img_path)\n",
    "                labels.append(emotion)\n",
    "        \n",
    "        print(f\"Created {len(image_paths)} synthetic image samples for training\")\n",
    "\n",
    "print(f\"\\nUnique emotions: {set(labels)}\")\n",
    "print(f\"Label distribution:\")\n",
    "from collections import Counter\n",
    "label_counts = Counter(labels)\n",
    "for emotion, count in label_counts.items():\n",
    "    print(f\"  {emotion}: {count}\")\n",
    "\n",
    "# Filter out non-existent image paths\n",
    "valid_paths = []\n",
    "valid_labels = []\n",
    "for path, label in zip(image_paths, labels):\n",
    "    if os.path.exists(path):\n",
    "        valid_paths.append(path)\n",
    "        valid_labels.append(label)\n",
    "\n",
    "image_paths = valid_paths\n",
    "labels = valid_labels\n",
    "print(f\"\\nValid image files: {len(image_paths)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e1de7b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "print(\"Sample Images:\")\n",
    "\n",
    "# Select a few images from each emotion\n",
    "sample_images = {}\n",
    "for emotion in EmotionUtils.EMOTION_LABELS:\n",
    "    emotion_paths = [path for path, label in zip(image_paths, labels) if label == emotion]\n",
    "    if emotion_paths:\n",
    "        sample_images[emotion] = emotion_paths[0]  # Take first image of each emotion\n",
    "\n",
    "if sample_images:\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (emotion, img_path) in enumerate(sample_images.items()):\n",
    "        if i < 8:  # Only show first 8\n",
    "            try:\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                if img is not None:\n",
    "                    axes[i].imshow(img, cmap='gray')\n",
    "                    axes[i].set_title(f'{emotion.capitalize()}')\n",
    "                    axes[i].axis('off')\n",
    "                else:\n",
    "                    axes[i].text(0.5, 0.5, f'{emotion}\\n(No image)', \n",
    "                               ha='center', va='center', transform=axes[i].transAxes)\n",
    "                    axes[i].axis('off')\n",
    "            except Exception as e:\n",
    "                axes[i].text(0.5, 0.5, f'{emotion}\\n(Error loading)', \n",
    "                           ha='center', va='center', transform=axes[i].transAxes)\n",
    "                axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Sample Images by Emotion')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No sample images available to display.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720f4c98",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "print(\"Creating data loaders...\")\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 32\n",
    "TEST_SIZE = 0.2\n",
    "IMAGE_SIZE = (48, 48)\n",
    "\n",
    "if len(image_paths) > 0:\n",
    "    train_loader, val_loader, label_encoder = data_loader.create_image_dataloaders(\n",
    "        image_paths=image_paths,\n",
    "        labels=labels,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        test_size=TEST_SIZE\n",
    "    )\n",
    "    \n",
    "    print(f\"Training batches: {len(train_loader)}\")\n",
    "    print(f\"Validation batches: {len(val_loader)}\")\n",
    "    print(f\"Classes: {label_encoder.classes_}\")\n",
    "else:\n",
    "    print(\"‚ùå No valid image data available for training\")\n",
    "    train_loader = None\n",
    "    val_loader = None\n",
    "    label_encoder = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ff5cbc",
   "metadata": {},
   "source": [
    "## 2. Initialize Model and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fae0d73",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize model trainer\n",
    "model_trainer = ModelTrainer(device=device)\n",
    "\n",
    "# Training configuration\n",
    "training_config = {\n",
    "    \"model_architecture\": \"MobileNetV2\",\n",
    "    \"num_epochs\": 10,  # Reduced for demo\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"image_size\": IMAGE_SIZE,\n",
    "    \"num_emotions\": len(EmotionUtils.EMOTION_LABELS),\n",
    "    \"device\": str(device),\n",
    "    \"pretrained\": True\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in training_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Save training config\n",
    "os.makedirs('../models/image_emotion_model', exist_ok=True)\n",
    "model_trainer.save_training_config(training_config, '../models/image_emotion_model/config.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12680bbe",
   "metadata": {},
   "source": [
    "## 3. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e633f30",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Train the image emotion model\n",
    "if train_loader is not None and val_loader is not None:\n",
    "    print(\"Starting model training...\")\n",
    "    \n",
    "    model_save_path = '../models/image_emotion_model/model.pth'\n",
    "    \n",
    "    history = model_trainer.train_image_model(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        model_save_path=model_save_path,\n",
    "        num_epochs=training_config[\"num_epochs\"],\n",
    "        learning_rate=training_config[\"learning_rate\"]\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Training completed!\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot start training - no valid data available\")\n",
    "    # Create a dummy history for demonstration\n",
    "    history = {\n",
    "        'train_loss': [0.8, 0.6, 0.4, 0.3, 0.2],\n",
    "        'val_loss': [0.9, 0.7, 0.5, 0.4, 0.35],\n",
    "        'train_acc': [60, 70, 80, 85, 90],\n",
    "        'val_acc': [55, 65, 75, 80, 82]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a80531",
   "metadata": {},
   "source": [
    "## 4. Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e86bc80",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "if 'history' in locals():\n",
    "    model_trainer.plot_training_history(\n",
    "        history, \n",
    "        save_path='../models/image_emotion_model/training_history.png'\n",
    "    )\n",
    "else:\n",
    "    print(\"No training history available to plot\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5615966f",
   "metadata": {},
   "source": [
    "## 5. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c6c33c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load and evaluate the trained model\n",
    "model_save_path = '../models/image_emotion_model/model.pth'\n",
    "\n",
    "if os.path.exists(model_save_path) and val_loader is not None:\n",
    "    # Load the best trained model\n",
    "    best_model = model_trainer.load_trained_model(model_save_path, \"image\")\n",
    "    \n",
    "    if best_model is not None:\n",
    "        print(\"Model loaded successfully!\")\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        print(\"\\nEvaluating model...\")\n",
    "        evaluation_results = model_trainer.evaluate_model(\n",
    "            model=best_model,\n",
    "            test_loader=val_loader,\n",
    "            model_type=\"image\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nValidation Accuracy: {evaluation_results['accuracy']:.4f}\")\n",
    "        \n",
    "        # Print classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        report = evaluation_results['classification_report']\n",
    "        for emotion in EmotionUtils.EMOTION_LABELS:\n",
    "            if emotion in report:\n",
    "                metrics = report[emotion]\n",
    "                print(f\"  {emotion}:\")\n",
    "                print(f\"    Precision: {metrics['precision']:.3f}\")\n",
    "                print(f\"    Recall: {metrics['recall']:.3f}\")\n",
    "                print(f\"    F1-score: {metrics['f1-score']:.3f}\")\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        model_trainer.plot_confusion_matrix(\n",
    "            evaluation_results['confusion_matrix'],\n",
    "            save_path='../models/image_emotion_model/confusion_matrix.png'\n",
    "        )\n",
    "    else:\n",
    "        print(\"‚ùå Failed to load trained model\")\n",
    "else:\n",
    "    print(\"‚ùå No trained model available or no validation data\")\n",
    "    best_model = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f173a8e4",
   "metadata": {},
   "source": [
    "## 6. Test Model with Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f222d7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Test the model with sample images\n",
    "if best_model is not None and len(image_paths) > 0:\n",
    "    print(\"Testing model with sample images:\\n\")\n",
    "    \n",
    "    # Select a few test images\n",
    "    test_indices = np.random.choice(len(image_paths), min(5, len(image_paths)), replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(test_indices), figsize=(15, 3))\n",
    "    if len(test_indices) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, idx in enumerate(test_indices):\n",
    "        img_path = image_paths[idx]\n",
    "        true_label = labels[idx]\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        try:\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is not None:\n",
    "                # Predict emotions\n",
    "                emotion_scores = model_trainer.predict_single_image(\n",
    "                    model=best_model,\n",
    "                    image=img\n",
    "                )\n",
    "                \n",
    "                # Get predicted emotion\n",
    "                predicted_emotion = max(emotion_scores.items(), key=lambda x: x[1])[0]\n",
    "                confidence = emotion_scores[predicted_emotion]\n",
    "                \n",
    "                # Display image\n",
    "                axes[i].imshow(img, cmap='gray')\n",
    "                axes[i].set_title(f'True: {true_label}\\nPred: {predicted_emotion}\\n({confidence:.2f})')\n",
    "                axes[i].axis('off')\n",
    "                \n",
    "                print(f\"Image {i+1}: {img_path}\")\n",
    "                print(f\"  True emotion: {true_label}\")\n",
    "                print(f\"  Predicted emotion: {predicted_emotion} (confidence: {confidence:.3f})\")\n",
    "                \n",
    "                # Show top 3 predictions\n",
    "                sorted_emotions = sorted(emotion_scores.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "                print(\"  Top predictions:\")\n",
    "                for emotion, score in sorted_emotions:\n",
    "                    emoji = EmotionUtils.get_emotion_emoji(emotion)\n",
    "                    print(f\"    {emoji} {emotion}: {score:.3f}\")\n",
    "                print()\n",
    "            else:\n",
    "                axes[i].text(0.5, 0.5, 'Failed to\\nload image', \n",
    "                           ha='center', va='center', transform=axes[i].transAxes)\n",
    "                axes[i].axis('off')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_path}: {e}\")\n",
    "            axes[i].text(0.5, 0.5, f'Error:\\n{str(e)[:20]}', \n",
    "                       ha='center', va='center', transform=axes[i].transAxes)\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ùå Model not available for testing or no images available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b6dee1",
   "metadata": {},
   "source": [
    "## 7. Model Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726cf9d3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Analyze model performance by emotion\n",
    "if 'evaluation_results' in locals():\n",
    "    print(\"Model Performance Analysis:\\n\")\n",
    "    \n",
    "    # Extract metrics for each emotion\n",
    "    emotions_data = []\n",
    "    report = evaluation_results['classification_report']\n",
    "    \n",
    "    for emotion in EmotionUtils.EMOTION_LABELS:\n",
    "        if emotion in report:\n",
    "            emotions_data.append({\n",
    "                'emotion': emotion,\n",
    "                'precision': report[emotion]['precision'],\n",
    "                'recall': report[emotion]['recall'],\n",
    "                'f1_score': report[emotion]['f1-score'],\n",
    "                'support': report[emotion]['support']\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame for easier analysis\n",
    "    if emotions_data:\n",
    "        performance_df = pd.DataFrame(emotions_data)\n",
    "        \n",
    "        print(\"Per-Emotion Performance:\")\n",
    "        print(performance_df.round(3))\n",
    "        \n",
    "        # Visualize performance metrics\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        # Precision\n",
    "        axes[0].bar(performance_df['emotion'], performance_df['precision'], \n",
    "                   color=[EmotionUtils.EMOTION_COLORS.get(e, '#888888') for e in performance_df['emotion']])\n",
    "        axes[0].set_title('Precision by Emotion')\n",
    "        axes[0].set_ylabel('Precision')\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Recall\n",
    "        axes[1].bar(performance_df['emotion'], performance_df['recall'],\n",
    "                   color=[EmotionUtils.EMOTION_COLORS.get(e, '#888888') for e in performance_df['emotion']])\n",
    "        axes[1].set_title('Recall by Emotion')\n",
    "        axes[1].set_ylabel('Recall')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # F1-Score\n",
    "        axes[2].bar(performance_df['emotion'], performance_df['f1_score'],\n",
    "                   color=[EmotionUtils.EMOTION_COLORS.get(e, '#888888') for e in performance_df['emotion']])\n",
    "        axes[2].set_title('F1-Score by Emotion')\n",
    "        axes[2].set_ylabel('F1-Score')\n",
    "        axes[2].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('../models/image_emotion_model/performance_analysis.png')\n",
    "        plt.show()\n",
    "        \n",
    "        # Best and worst performing emotions\n",
    "        best_f1 = performance_df.loc[performance_df['f1_score'].idxmax()]\n",
    "        worst_f1 = performance_df.loc[performance_df['f1_score'].idxmin()]\n",
    "        \n",
    "        print(f\"\\nüèÜ Best performing emotion: {best_f1['emotion']} (F1: {best_f1['f1_score']:.3f})\")\n",
    "        print(f\"üî¥ Worst performing emotion: {worst_f1['emotion']} (F1: {worst_f1['f1_score']:.3f})\")\n",
    "        \n",
    "        # Overall metrics\n",
    "        if 'macro avg' in report:\n",
    "            macro_avg = report['macro avg']\n",
    "            print(f\"\\nüìä Overall Performance:\")\n",
    "            print(f\"   Macro Precision: {macro_avg['precision']:.3f}\")\n",
    "            print(f\"   Macro Recall: {macro_avg['recall']:.3f}\")\n",
    "            print(f\"   Macro F1-Score: {macro_avg['f1-score']:.3f}\")\n",
    "    else:\n",
    "        print(\"No emotion-specific performance data available\")\n",
    "else:\n",
    "    print(\"No evaluation results available for analysis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecde7968",
   "metadata": {},
   "source": [
    "## 8. Save Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da0ba64",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create model summary\n",
    "model_summary = {\n",
    "    \"model_type\": \"Image Emotion Recognition\",\n",
    "    \"architecture\": \"MobileNetV2 + Classification Head\",\n",
    "    \"training_data\": {\n",
    "        \"total_samples\": len(image_paths) if 'image_paths' in locals() else 0,\n",
    "        \"training_samples\": len(train_loader.dataset) if train_loader else 0,\n",
    "        \"validation_samples\": len(val_loader.dataset) if val_loader else 0,\n",
    "        \"emotions\": list(EmotionUtils.EMOTION_LABELS),\n",
    "        \"image_size\": IMAGE_SIZE\n",
    "    }\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
